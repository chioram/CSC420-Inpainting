{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Inpainting NN Final.ipynb","provenance":[{"file_id":"1wwTDGKBxdFwC6DIyn9U9-DK8GRjkkxUX","timestamp":1607466945870}],"collapsed_sections":[],"authorship_tag":"ABX9TyOLnI3eN6wTYDSUspPEnfO+"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"Jqpfvpnl6ns4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1_61HGT6Lat","executionInfo":{"status":"ok","timestamp":1610642335130,"user_tz":300,"elapsed":3965,"user":{"displayName":"Chiora Mba-Uzoukwu","photoUrl":"","userId":"14172680014304765067"}},"outputId":"28378fde-9cd9-4d4d-b8dd-19ce2ceb627a"},"source":["!pip3 install torch torchvision"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T2cTp0pmCAr1"},"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torchvision.datasets import ImageFolder\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","import os\n","import time\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gXjtSDHoB-l","executionInfo":{"status":"ok","timestamp":1610642360993,"user_tz":300,"elapsed":29820,"user":{"displayName":"Chiora Mba-Uzoukwu","photoUrl":"","userId":"14172680014304765067"}},"outputId":"75f743f6-0222-4706-8afb-543a4bec2cd6"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XjHzYtQOg649"},"source":["IMG_DIMS_X = 256\n","IMG_DIMS_Y = 256"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NKkjWV7b60S"},"source":["args = { # adapted from tutorial\n","    'gpu': True,\n","    'valid':False, \n","    'checkpoint':\"\", \n","    'model':\"CNN\", \n","    'kernel':3, \n","    'num_filters':32, # tweak.\n","    'learn_rate':0.001, # tweak\n","    'batch_size':32, # <-- tweak\n","    'epochs':25, # tweak\n","    'seed':0, \n","    'plot':True, \n","    'experiment_name': 'cnn_autoencoder',\n","    'visualize': True,\n","    'downsize_input':False,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJRZDdr3kzne"},"source":["data_root = '/content/gdrive/MyDrive/segmented data'\n","data_root_val = '/content/gdrive/MyDrive/further segments'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qebQqc2nf7if"},"source":["def plot(masked, inpainted, truth, fpath, peek_idx=0):\n","\n","  \"\"\" this function takes a lot of time, aim to plot less \"\"\"\n","  if peek_idx is None:\n","    peek_idx = np.random.randint(0,masked.size()[0])\n","\n","  \n","  masked = masked.permute(2,3,1,0)\n","  masked = masked[:,:,:,peek_idx].reshape(-1,IMG_DIMS_Y,3)\n","  inpainted = inpainted.permute(2,3,1,0)\n","  inpainted = inpainted[:,:,:,peek_idx].reshape(-1,IMG_DIMS_Y,3)\n","  truth = truth.permute(2,3,1,0)\n","  truth = truth[:,:,:,peek_idx].reshape(-1,IMG_DIMS_Y,3)\n","\n","  print(masked.size(), inpainted.size(), truth.size())\n","\n","\n","  canvas = torch.cat((masked, inpainted, truth), 1)\n","  canvas = (canvas* 255).clamp(0, 255)\n","\n","  canvas = canvas.cpu().numpy().astype(np.uint8)\n","  plt.imshow(canvas)\n","\n","  # save here if neeeded\n","  if fpath is not None:\n","    pass\n","\n","  plt.show()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4a--8_QLpbt"},"source":["def inpaint(img_masked, prediction, mask):\n","\n","  \"\"\"Returns the image, but with the neural net's prediction filled into the holes.\n","  This represents the final and actual inpainted result of the net.\"\"\"\n","\n","  holes = mask == 0\n","  holes = holes.permute((0,3,2,1))\n","\n","  img_masked_copy = img_masked.clone()\n","\n","  img_masked_copy[holes] = prediction[holes]\n","  \n","  return img_masked_copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8gWD5rjeHfj"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gApVAYmElCpJ"},"source":["class Modified_MSE(nn.MSELoss):\n","\n","  \"\"\"Loss function to score the accuracy of an image's reprouction.\n","      Considers how faithfully the reconstruction mimics the masked region.\"\"\"\n","\n","  def __init__(self, size_average=None, reduce=None, reduction: str = 'mean'):\n","    super(Modified_MSE, self).__init__(size_average, reduce, reduction)\n","\n","  def forward(self, output, img, img_masked, mask):\n","        # two separate losses, calculate individually then avg\n","        \n","        zeros_mask = mask == 0\n","        zeros_mask = zeros_mask.permute((0,3,1,2))\n","\n","        whites_mask = mask == 255\n","        whites_mask = whites_mask.permute((0,3,1,2))\n","        \n","        loss_zeros = F.mse_loss(output[zeros_mask], img[zeros_mask], reduction=self.reduction)\n","\n","        return loss_zeros\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9Dqgq3SgLgm"},"source":["class Places(ImageFolder):\n","\n","  \"\"\" A subsample of the Places Small Dataset. Includes only Nature images.\"\"\"\n","\n","  def __init__(self, root, transform=None, fixed_mask=False):\n","    super(Places, self).__init__(root, transform)\n","    self.fixed_mask = fixed_mask\n","    if self.fixed_mask:\n","      self.masks = []\n","      for i in range(0,self.__len__()):\n","        # print(i)\n","        if i < 1600:\n","          tup = self._create_mask(self._getimg(i))\n","          self.masks += [(tup[0], tup[1])]\n","        else:\n","          tup = self._mask_with_exisiting(self._getimg(i), self.masks[i % 1600][1])\n","          self.masks += [(tup[0], tup[1])]\n","\n","  def _create_mask(self, img):\n","    \"\"\"returns masked img from dataset along with mask\n","    \n","    Try with both random masks per batch load and fixed masks across all batch loads.\"\"\"\n","\n","    mask = np.full((IMG_DIMS_X, IMG_DIMS_Y, 3), 255, np.uint8) \n","    for _ in range(np.random.randint(3, 5)):\n","      x1, x2 = np.random.randint(1, IMG_DIMS_X), np.random.randint(1, IMG_DIMS_X)\n","      y1, y2 = np.random.randint(1, IMG_DIMS_Y), np.random.randint(1, IMG_DIMS_Y)\n","      thickness = np.random.randint(5,10)\n","\n","      cv2.line(mask,(x1,y1),(x2,y2),(0,0,0),thickness)\n","\n","    masked = img.clone()\n","    masked = (masked* 255).clamp(0, 255)\n","    masked = np.array(masked, np.uint8)\n","\n","    mask2 = np.transpose(mask, (2,1,0))\n","    \n","    masked[mask2 == 0] = 255\n","    # masked is an np.array when returned\n","\n","    return masked, mask\n","  \n","  def _mask_with_exisiting(self, img, mask):\n","\n","    \"assumes mask in tensor dim order\"\n","\n","    masked = img.clone()\n","    masked = (masked* 255).clamp(0, 255)\n","    masked = np.array(masked, np.uint8)\n","    mask2 = np.transpose(mask, (2,1,0))\n","    masked[mask2 == 0] = 255\n","\n","    return masked, mask\n","\n","\n","  def _getimg(self, index):\n","    img, label = super().__getitem__(index)\n","    return img\n","\n","  def __getitem__(self, index):\n","    img, label = super().__getitem__(index)\n","\n","    if self.fixed_mask:\n","      masked_img, mask = self.masks[index]\n","      masked_img = np.transpose(masked_img, (1, 2, 0))\n","    else:\n","      masked_img, mask = self._create_mask(img)\n","      masked_img = np.transpose(masked_img, (1, 2, 0))\n","\n","    masked_img = self.transform(masked_img)\n","\n","    return img, masked_img, mask\n","\n","  def __len__(self):\n","    return super().__len__()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQfir8BD_9uJ"},"source":[""]},{"cell_type":"code","metadata":{"id":"T2CrRdnwg1xR"},"source":["num_in_channels = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNMpNmC4Cl1O"},"source":["class ConvAutoencoder(nn.Module):\n","\n","  def __init__(self, kernel, num_filters, num_in_channels):\n","    # can experiment with different architectures in the future\n","\n","    super(ConvAutoencoder,self).__init__()\n","\n","    padding = kernel // 2\n","    self.down1 = nn.Sequential(\n","        nn.Conv2d(num_in_channels, num_filters, kernel_size=kernel, padding=padding),\n","        nn.BatchNorm2d(num_filters),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),)\n","    self.down2 = nn.Sequential(\n","        nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n","        nn.BatchNorm2d(num_filters*2),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),)\n","\n","    self.rfconv = nn.Sequential(\n","        nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),\n","        nn.BatchNorm2d(num_filters*2),\n","        nn.ReLU())\n","\n","    self.up1 = nn.Sequential(\n","        nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n","        nn.BatchNorm2d(num_filters),\n","        nn.ReLU(),\n","        nn.Upsample(scale_factor=2),)\n","    self.up2 = nn.Sequential(\n","        nn.Conv2d(num_filters*2, 3, kernel_size=kernel, padding=padding),\n","        nn.BatchNorm2d(3),\n","        nn.ReLU(),\n","        nn.Upsample(scale_factor=2),)\n","  \n","\n","\n","  def forward(self, x):\n","    self.out1 = self.down1(x)\n","    self.out2 = self.down2(self.out1)\n","    self.out3 = self.rfconv(self.out2)\n","    self.out4 = self.up1(self.out3)\n","    self.out5 = self.up2(self.out4)\n","    self.out_final = self.out5\n","    return self.out_final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpdZaMrFzNn2"},"source":["def run_validation_step(cnn, criterion, val_dloader, batch_size, plotpath=None, visualize=True, downsize_input=False):\n","    correct = 0.0\n","    total = 0.0\n","    losses = []\n","    num_colours = 3\n","    \n","    for i, (x_gr_truth, x_masked, mask) in enumerate(train_loader):\n","        x_gr_truth, x_masked, mask = x_gr_truth.cuda(), x_masked.cuda(), mask.cuda()\n","        outputs = cnn(x_masked)\n","\n","        val_loss = criterion(outputs, x_gr_truth, x_masked, mask)\n","        losses.append(val_loss.data.item())\n","\n","    result = inpaint(x_masked.detach(), outputs.detach(), mask.detach())\n"," \n","    if plotpath: \n","        plot(x_masked.detach(), result.detach(), x_gr_truth.detach(), plotpath, None)\n","\n","    val_loss = np.mean(losses)\n","    return val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eoQsO8OhFiv"},"source":["def train(args, cnn=None):\n","\n","  torch.set_num_threads(5)\n","  save_dir = 'outputs/' + args['experiment_name']\n","\n","  num_in_channels = 3 \n","\n","  if cnn is None:\n","    cnn = ConvAutoencoder(args['kernel'], args['num_filters'], num_in_channels)\n","\n","  optimizer = torch.optim.Adam(cnn.parameters(), lr=args['learn_rate'])\n","\n","  # load data\n","\n","  train_set = Places(data_root, transform=transform, fixed_mask=True)\n","  train_loader = torch.utils.data.DataLoader(train_set, batch_size=args['batch_size'])\n","\n","  val_set = Places(data_root_val, transform=transform, fixed_mask=True) \n","  val_loader = torch.utils.data.DataLoader(val_set, batch_size=args['batch_size'])\n","\n","  if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","  # start training \n","\n","  print(\"Training...\")\n","  if args['gpu']: \n","    cnn.cuda()\n","  start = time.time()\n","\n","  train_losses = []\n","  val_losses = []\n","\n","  for epoch in range(args['epochs']):\n","        cnn.train() # Change model to 'train' mode\n","        losses = []\n","        for i, (x_gr_truth, x_masked, mask) in enumerate(train_loader):\n","            x_gr_truth, x_masked, mask = x_gr_truth.cuda(), x_masked.cuda(), mask.cuda()\n","            # NOTE: tensors are passed by reference\n","\n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","\n","            outputs = cnn(x_masked) \n","            criterion = Modified_MSE() \n","\n","            loss = criterion(outputs, x_gr_truth, x_masked, mask)\n","\n","            loss.backward()\n","            optimizer.step()\n","            losses.append(loss.data.item())\n","            result = inpaint(x_masked.detach(), outputs.detach(), mask.detach())\n","        # plot training images\n","        if args['plot'] and epoch % 3 == 0:\n","            \n","            plot(x_masked.detach(), result.detach(), x_gr_truth.detach(), save_dir+'/train_%d.png' % epoch)\n","\n","        # log training losses\n","        avg_loss = np.mean(losses)\n","        train_losses.append(avg_loss)\n","        time_elapsed = time.time() - start\n","        print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n","            epoch+1, args['epochs'], avg_loss, time_elapsed))\n","\n","        # Evaluate the model\n","        cnn.eval()  # Change model to 'eval' mode.\n","        val_loss = run_validation_step(cnn,\n","                                                Modified_MSE(),\n","                                                val_loader,\n","                                                args['batch_size'],\n","                                                save_dir+'/test_%d.png' % epoch,\n","                                                args['visualize'],\n","                                                args['downsize_input'])\n","\n","        time_elapsed = time.time() - start\n","        val_losses.append(val_loss)\n","        print('Epoch [%d/%d], Val Loss: %.4f, Time(s): %d' % (\n","            epoch+1, args['epochs'], val_loss, time_elapsed))\n","\n","  if args['checkpoint']:\n","    print('Saving model...')\n","    torch.save(cnn.state_dict(), args['checkpoint'])\n","    \n","  return cnn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"BGGyS1u2IqBY","outputId":"642f8906-7272-4418-b07a-68e61be6c121"},"source":["cnn = train(args) "],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-5bfd8bfed8d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-8f256108f294>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, cnn)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-dd942797a8f6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, fixed_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask_with_exisiting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-dd942797a8f6>\u001b[0m in \u001b[0;36m_getimg\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_getimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[1;32m    150\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Leh1hJmEuj0s"},"source":[""],"execution_count":null,"outputs":[]}]}